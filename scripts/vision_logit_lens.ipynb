{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b204e773b48f54",
   "metadata": {},
   "outputs": [],
   "source": "import os\n# Set CUDA_VISIBLE_DEVICES to use only GPU 2\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n\nimport copy\nimport os.path\nimport pickle\nfrom os.path import join\nimport sys\n\nimport cv2\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import ImageDraw\nfrom matplotlib import cm\n\nimport jax\nimport jax.experimental\nimport jax.numpy as jnp\nfrom PIL import Image\n\n# Import from openpi package (already in Python path)\nfrom openpi import EXP_DATA_PATH\nfrom openpi.models.pi0 import Pi0\nimport openpi.models.model as _model\nimport openpi.training.config as _config\nfrom openpi.shared.download import maybe_download\n\n# Import from scripts/text_latent.py\nimport sys\nscripts_path = os.path.abspath('.')\nif scripts_path not in sys.path:\n    sys.path.append(scripts_path)\n    \nfrom text_latent import Checkpoint, Args, create_dataloader\n\n\ndef restore_img(img) -> Image:\n    float_img = np.asarray(img + 1, dtype=np.float64) / 2 * 255\n    int_img = float_img.astype(np.uint8)\n    img = Image.fromarray(int_img)\n    return img\n\n\ndef normalize(vectors):\n    return vectors / jnp.linalg.norm(vectors, axis=-1, keepdims=True)\n\n\ndef patch_idx_to_hw(patch_idx, num_patch=16, patch_size=14):\n    \"\"\"Convert patch index to height and width.\"\"\"\n    h = patch_idx // num_patch\n    w = patch_idx % num_patch\n    return int(h * patch_size), int(w * patch_size)\n\n\ndef draw_box_on_image(image, top_left_corner, box_width=14, box_height=14, color='red', thickness=1):\n    # Create a Draw object to modify the image\n    draw = ImageDraw.Draw(image)\n\n    # Extract coordinates and calculate the bounding box\n    top = top_left_corner[0]\n    left = top_left_corner[1]\n    right = left + box_width\n    bottom = top + box_height\n\n    bounding_box = (left, top, right, bottom)\n\n    # Draw the rectangle\n    draw.rectangle(bounding_box, outline=color, width=thickness)\n\n    return image"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb37027b698e269",
   "metadata": {},
   "outputs": [],
   "source": "# Check if EXP_DATA_PATH exists, create if not\nif not os.path.exists(EXP_DATA_PATH):\n    os.makedirs(EXP_DATA_PATH, exist_ok=True)\n    print(f\"Created directory: {EXP_DATA_PATH}\")\n\n# load text latents for libero goal task\ntext_latent_path = join(EXP_DATA_PATH, \"pi0\")\nlibero_goal_task_id = [i for i in range(10, 40)]\ntext_latents = {}\n\n# Check if text latent files exist\nif os.path.exists(text_latent_path):\n    for id in libero_goal_task_id:\n        file_path = join(text_latent_path, f\"avg_states_{id}_{id + 1}_frame_0_119.pkl\")\n        if os.path.exists(file_path):\n            with open(file_path, \"rb\") as f:\n                data = pickle.load(f)\n            text_latents[id] = data[\"hidden_states_avg\"]\n        else:\n            print(f\"Warning: Text latent file not found: {file_path}\")\nelse:\n    print(f\"Warning: Text latent directory not found: {text_latent_path}\")\n    print(\"Please run text_latent.py first to generate text latents\")\n\n# load training data for libero goal task\npolicy = \"pi0\"\nargs = Args()\nargs.policy = Checkpoint(config=f\"{policy}_libero\", dir=\"s3://openpi-assets/checkpoints/{}_libero\".format(policy))\n\ntry:\n    train_config = _config.get_config(args.policy.config)\n    data_config = train_config.data.create(train_config.assets_dirs, train_config.model)\n    ckpt_dir = maybe_download(args.policy.dir)\n\n    # load model\n    model: Pi0 = train_config.model.load(_model.restore_params(ckpt_dir / \"params\", dtype=jnp.bfloat16))\n    embed_prefix_jit = jax.jit(model.embed_prefix)\n    \n    def embed_prefix(observation):\n        observation = _model.preprocess_observation(None, observation, train=False)\n        return embed_prefix_jit(observation)\n    \n    print(\"Model loaded successfully\")\nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    model = None\n    embed_prefix_jit = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d132058c4470ce9c",
   "metadata": {},
   "outputs": [],
   "source": "# Load observation data for visualization\nall_obs = []\n\n# Only proceed if model is loaded successfully\nif model is not None and embed_prefix_jit is not None:\n    for task_id in range(10, 40):\n        try:\n            task_range = (task_id, task_id + 1)\n            episode_to_use_for_collection = 1\n            data_loader, dataset_meta = create_dataloader(train_config, data_config, ckpt_dir, task_range, 1,\n                                                          episode_to_use_for_collection, True)\n            obs = [_model.Observation.from_dict(element) for element in list(iter(data_loader))]\n            base_images = [obs[i].images[\"base_0_rgb\"][0] for i in range(len(obs))]\n            print(f\"Task {task_id}: {dataset_meta.tasks[task_id]}\")\n\n\n            def decode(obs):\n                if not isinstance(obs, list):\n                    obs = obs.tokenized_prompt[0].tolist()\n                return data_loader.torch_loader.dataset._transform.transforms[-1].tokenizer._tokenizer.decode(obs)\n\n\n            prompt = obs[0].tokenized_prompt[0]\n            decoded = [decode([token.item()]) for token in prompt]\n            prompt = decoded[:obs[0].tokenized_prompt_mask.sum()]\n            print(f\"Tokenized prompt: {prompt}\")\n            print(f\"Token index: {prompt[:obs[0].tokenized_prompt_mask.sum()]}\")\n\n            # check embedding same\n            prompt_len = len(decoded[:obs[0].tokenized_prompt_mask.sum()])\n            all_obs.append((obs, len(prompt), prompt, len(obs)))\n            \n        except Exception as e:\n            print(f\"Error loading data for task {task_id}: {e}\")\n            continue\n            \n    print(f\"Successfully loaded {len(all_obs)} tasks\")\nelse:\n    print(\"Model not loaded. Cannot proceed with data loading.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b1ee8c3e30cad2",
   "metadata": {},
   "outputs": [],
   "source": "# Attention visualization functions\ndef get_max_sim_patch(task_start, task_id, all_obs, obs_idx=0):\n    \"\"\"Compute similarity between text latent and image patches.\"\"\"\n    if not text_latents or task_id not in text_latents:\n        print(f\"Text latent not found for task {task_id}\")\n        return None\n        \n    if task_id - task_start >= len(all_obs):\n        print(f\"Task {task_id} not found in all_obs\")\n        return None\n        \n    task_obs = copy.deepcopy(all_obs[task_id - task_start])\n    obs = task_obs[0]\n    prompt_len = task_obs[1]\n    \n    if obs_idx >= len(obs):\n        print(f\"Observation index {obs_idx} out of range for task {task_id}\")\n        return None\n        \n    episode_similarity = jnp.zeros((18, 256), dtype=jnp.float32)\n\n    try:\n        # Get image and text embeddings\n        embeddings, _, _ = embed_prefix(obs[obs_idx])\n        image_embedding = embeddings[0, :256]\n        text_latent = text_latents[task_id][:, 256 * 3 + 3:256 * 3 + prompt_len].sum(-2)\n        \n        # Normalize embeddings\n        normed_image_embedding = normalize(image_embedding)\n        normed_text_latent = normalize(text_latent)\n\n        # Compute similarity\n        similarity = jnp.dot(normed_text_latent, normed_image_embedding.T)\n        episode_similarity += similarity\n\n        patch_episode_similarity = jnp.max(episode_similarity[1:], axis=0)\n\n        # Generate visualization\n        float_img = np.asarray(task_obs[0][obs_idx].images[\"base_0_rgb\"][0] + 1, dtype=np.float64) / 2 * 255\n        original_image = float_img.astype(np.uint8)\n        correlate_map = np.asarray(patch_episode_similarity).reshape(16, 16)\n        norm_attention_map = (correlate_map - correlate_map.min()) / (correlate_map.max() - correlate_map.min())\n        resized_attention_map = cv2.resize(norm_attention_map, (224, 224), interpolation=cv2.INTER_LINEAR)\n        heatmap_rgba = cm.jet(resized_attention_map)\n        heatmap_bgr = (heatmap_rgba[:, :, :3] * 255).astype(np.uint8)\n        alpha = 0.3\n        blended_image = cv2.addWeighted(heatmap_bgr, alpha, original_image, 1 - alpha, 0)\n        return Image.fromarray(blended_image)\n    \n    except Exception as e:\n        print(f\"Error in get_max_sim_patch for task {task_id}, obs {obs_idx}: {e}\")\n        return None\n\n\ndef plot_dynamics(task_id):\n    \"\"\"Plot attention dynamics over time for a given task.\"\"\"\n    if task_id - 10 >= len(all_obs):\n        print(f\"Task {task_id} not found in loaded observations\")\n        return\n        \n    task_obs = all_obs[task_id - 10]\n    print(f\"Task {task_id}: {task_obs[2]}\")\n    \n    # Generate images for different time steps\n    images = []\n    max_obs = min(task_obs[-1], 100)  # Limit to avoid memory issues\n    step_size = max(1, max_obs // 20)  # Generate ~20 images max\n    \n    for t in range(0, max_obs, step_size):\n        img = get_max_sim_patch(10, task_id, all_obs, obs_idx=t)\n        if img is not None:\n            images.append(img)\n        else:\n            print(f\"Skipping timestep {t} due to error\")\n\n    if not images:\n        print(\"No valid images generated\")\n        return\n        \n    print(f\"Generated {len(images)} attention visualizations\")\n\n    def show_image(index):\n        if 0 <= index < len(images):\n            plt.figure(figsize=(8, 8))\n            plt.imshow(images[index])\n            plt.title(f\"Task {task_id} - Timestep {index * step_size}\")\n            plt.axis('off')\n            plt.show()\n        else:\n            print(f\"Invalid index: {index}\")\n\n    # Create interactive slider\n    slider = widgets.IntSlider(\n        value=0, \n        min=0, \n        max=len(images) - 1, \n        step=1, \n        description='Timestep:'\n    )\n    widgets.interact(show_image, index=slider)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d00ac508f74f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamics(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f26d7d7f788e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamics(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070adb31132cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamics(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a6f17fcbd00184",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dynamics(27)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3vj0vd1e46g",
   "source": "# Quick test to verify everything is working\nprint(\"=== Testing VLM Attention Visualization ===\")\nprint(f\"EXP_DATA_PATH: {EXP_DATA_PATH}\")\nprint(f\"Text latents loaded for {len(text_latents)} tasks\")\nprint(f\"Observation data loaded for {len(all_obs) if 'all_obs' in globals() else 0} tasks\")\n\nif model is not None:\n    print(\"✓ Model loaded successfully\")\nelse:\n    print(\"✗ Model not loaded\")\n    \nif text_latents:\n    print(f\"✓ Text latents available for tasks: {sorted(text_latents.keys())}\")\nelse:\n    print(\"✗ No text latents loaded\")\n\n# Test a single attention visualization if everything is loaded\nif model is not None and text_latents and 'all_obs' in globals() and all_obs:\n    print(\"\\n=== Testing attention visualization for Task 22 ===\")\n    try:\n        test_img = get_max_sim_patch(10, 22, all_obs, obs_idx=0)\n        if test_img is not None:\n            print(\"✓ Attention visualization test successful!\")\n            # Display the test image\n            plt.figure(figsize=(8, 8))\n            plt.imshow(test_img)\n            plt.title(\"Test: Task 22 Attention Visualization\")\n            plt.axis('off')\n            plt.show()\n        else:\n            print(\"✗ Attention visualization test failed\")\n    except Exception as e:\n        print(f\"✗ Attention visualization test error: {e}\")\nelse:\n    print(\"\\n✗ Cannot test attention visualization - missing components\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}